# -*- coding: utf-8 -*-
"""Proyek_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PHxDrITPuMJRfB4N_oqSFf8sdwbHyFWy

Install kaggle API untuk download dataset
"""

! pip install -q kaggle

from google.colab import files

files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d CooperUnion/anime-recommendations-database

! unzip /content/anime-recommendations-database.zip

! pip install fuzzywuzzy

"""import library yang diperlukan"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from fuzzywuzzy import fuzz
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity

"""Helper Function"""

# Function untuk memisahkan tiap genre
def comma_tokenizer(s):
   return s.split(', ')

# create a function to find the closest title
def matching_score(a,b):
   return fuzz.ratio(a,b)

# fuzz.ratio(a,b) calculates the Levenshtein Distance
# between a and b, and returns the score for the distance. 
# If the two words, a and b, are exactly the same, the score becomes 100. 
# As the distance between the words increases, the score falls.

"""# Data Understanding

berikut adalah penjelasan tiap kolom untuk seluruh data

Anime.csv

* anime_id - myanimelist.net's unique id identifying an anime.
* name - full name of anime.
* genre - comma separated list of genres for this anime.
* type - movie, TV, OVA, etc.
* episodes - how many episodes in this show. (1 if movie).
* rating - average rating out of 10 for this anime.
* members - number of community members that are in this anime's
"group".


Rating.csv

* user_id - non identifiable randomly generated user id.
* anime_id - the anime that this user has rated.
* rating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).

Melakukan load data
"""

df_anime = pd.read_csv('/content/anime.csv')
df_anime

df_anime.info()

"""mengecek missing values"""

df_anime.isna().sum()

df_anime.dropna(inplace=True)
df_anime.isna().sum()

"""Melihat persebaran tipe anime"""

df_anime.type.value_counts()

df_anime.type.value_counts().plot(kind='pie', autopct='%1.1f%%')

"""melihat persebaran rating seluruh anime"""

df_anime.rating.plot(kind='hist')
plt.title('Rating')
plt.show()

"""membuat visualisasi persebaran genre seluruh anime"""

counts = dict()
for i in df_anime.index:
   for g in df_anime.loc[i,'genre'].split(', '):
      if g not in counts:
         counts[g] = 1
      else:
         counts[g] = counts[g] + 1
# create a bar chart
plt.figure(figsize=(10, 3))
plt.bar(list(counts.keys()), counts.values(), color='g')
plt.xticks(rotation=90)
plt.xlabel('Genres')
plt.ylabel('Counts')

counts

df_anime.describe()

len(df_anime)

"""Menghapus anime yang tidak bertipe TV (Serial)"""

df_anime = df_anime[df_anime['type'] == 'TV']
df_anime.reset_index(drop=True, inplace = True)
df_anime

"""membuat visualisasi persebaran genre anime bertipe TV/Serial"""

counts = dict()
for i in df_anime.index:
   for g in df_anime.loc[i,'genre'].split(', '):
      if g not in counts:
         counts[g] = 1
      else:
         counts[g] = counts[g] + 1
# create a bar chart
plt.figure(figsize=(10, 3))
plt.bar(list(counts.keys()), counts.values(), color='g')
plt.xticks(rotation=90)
plt.xlabel('Genres')
plt.ylabel('Counts')

counts

"""Load data rating"""

df_rating = pd.read_csv('/content/rating.csv')
df_rating

df_rating.info()

"""mengecek missing values"""

df_rating.isna().sum()

df_rating.describe()

"""menghapus data anime yang belum pernah dirating dan hanya ditonton user (nilai rating -1)"""

df_rating = df_rating[df_rating['rating'] >= 0]
df_rating = df_rating[df_rating['anime_id'].isin(df_anime['anime_id'].tolist())]
df_rating.reset_index(drop=True, inplace = True)
df_rating

df_rating.describe()

print('Jumlah user_id: ', len(df_rating.user_id.unique()))
print('Jumlah anime_id: ', len(df_rating.anime_id.unique()))
print('Jumlah data rating: ', len(df_rating))

"""# Content Based Filtering

Menghapus kolom yang tidak diperlukan untuk content based filtering
"""

df_anime_clean = df_anime.drop(columns=['anime_id','type', 'episodes', 'rating', 'members'], axis=1)
df_anime_clean.reset_index(drop=True, inplace = True)
df_anime_clean

"""melakukan fitting TF-IDF untuk kolom genre"""

# create an object for TfidfVectorizer
tfidf_vector = TfidfVectorizer(tokenizer=comma_tokenizer)
# apply the object to the genres column
tfidf_matrix = tfidf_vector.fit_transform(df_anime_clean['genre'])

tfidf_matrix.shape

print(list(enumerate(tfidf_vector.get_feature_names())))

"""membuat matrix cosine similarity"""

# create the cosine similarity matrix
sim_matrix = cosine_similarity(tfidf_matrix) 
print(sim_matrix)

# a function to convert index to name
def get_name_from_index(index):
   return df_anime_clean[df_anime_clean.index == index]['name'].values[0]
# a function to convert title to index
def get_index_from_name(name):
   return df_anime_clean[df_anime_clean.name == name].index.values[0]
def get_genre_from_index(index):
  return df_anime_clean[df_anime_clean.index == index]['genre'].values[0]
# a function to return the most similar title to the words a user type
def find_closest_title(name):
   leven_scores = list(enumerate(df_anime_clean['name'].apply(matching_score, b=name)))
   sorted_leven_scores = sorted(leven_scores, key=lambda x: x[1], reverse=True)
   closest_name = get_name_from_index(sorted_leven_scores[0][0])
   distance_score = sorted_leven_scores[0][1]
   return closest_name, distance_score

def contents_based_recommender(anime_user_likes, how_many):
   closest_title, distance_score = find_closest_title(anime_user_likes)
   # When a user does not make misspellings
   if distance_score == 100:
      movie_index = get_index_from_name(closest_title)
      movie_list = list(enumerate(sim_matrix[int(movie_index)]))
      # remove the typed movie itself
      similar_anime = list(filter(lambda x:x[0] != int(movie_index), sorted(movie_list,key=lambda x:x[1], reverse=True))) 
      
      print('Here\'s the list of anime similar to '+'\033[1m'+str(closest_title)+'\033[0m : '+get_genre_from_index(movie_index)+'.\n')
      for i,s in similar_anime[:how_many]:
         print(get_name_from_index(i), ": ",get_genre_from_index(i))
   # When a user makes misspellings    
   else:
      print('Did you mean '+'\033[1m'+str(closest_title)+'\033[0m'+'?','\n')
      movie_index = get_index_from_name(closest_title)
      movie_list = list(enumerate(sim_matrix[int(movie_index)]))
      similar_anime = list(filter(lambda x:x[0] != int(movie_index), sorted(movie_list,key=lambda x:x[1], reverse=True)))
      print('Here\'s the list of anime similar to '+'\033[1m'+str(closest_title)+'\033[0m : '+get_genre_from_index(movie_index)+'.\n')
      for i,s in similar_anime[:how_many]:
         print(get_name_from_index(i), ": ",get_genre_from_index(i))

"""mencari rekomendasi anime sejenis Naruto"""

contents_based_recommender('Naruto', 10)

"""Output Jika terjadi typo"""

contents_based_recommender('Baruto', 10)

"""# Collaborative Filtering

"""

df = df_rating
df

"""encoding user_id"""

# Mengubah user_id menjadi list tanpa nilai yang sama
user_ids = df['user_id'].unique().tolist()
print('list user_id: ', user_ids)
 
# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

"""encoding anime_id"""

# Mengubah anime_id menjadi list tanpa nilai yang sama
anime_ids = df['anime_id'].unique().tolist()
 
# Melakukan proses encoding anime_id
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
 
# Melakukan proses encoding angka ke anime_id
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

# Mapping user_id ke dataframe user
df['user'] = df['user_id'].map(user_to_user_encoded)
 
# Mapping anime_id ke dataframe
df['anime'] = df['anime_id'].map(anime_to_anime_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah anime
num_anime = len(anime_encoded_to_anime)
print(num_anime)
 
# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['rating'])
 
# Nilai maksimal rating
max_rating = max(df['rating'])
 
print('Number of User: {}, Number of Anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""mengacak dataset"""

df = df.sample(frac=1, random_state=42)
df

"""melakukan train-test-split"""

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = df[['user', 'anime']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 8900% data train dan 10% data validasi
train_indices = int(0.9 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.anime_embedding = layers.Embedding( # layer embeddings anime
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1) # layer embedding anime bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embedding(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2) 
 
    x = dot_user_anime + user_bias + anime_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_anime, 64) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.RMSprop(),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 256,
    epochs = 15,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
anime_watched_by_user = df[df.user_id == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
anime_not_watched_by_user = df_anime[~df_anime['anime_id'].isin(anime_watched_by_user.anime_id.values)]['anime_id'] 
anime_not_watched_by_user = list(
    set(anime_not_watched_by_user)
    .intersection(set(anime_to_anime_encoded.keys()))
)
 
anime_not_watched_by_user = [[anime_to_anime_encoded.get(x)] for x in anime_not_watched_by_user]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched_by_user), anime_not_watched_by_user)
)

ratings = model.predict(user_anime_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_watched_by_user[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Anime with high ratings from user')
print('----' * 8)
 
top_anime_user = (
    anime_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(10)
    .anime_id.values
)
 
anime_df_rows = df_anime[df_anime['anime_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.name, ':', row.genre)
 
print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)
 
recommended_anime = df_anime[df_anime['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.name, ':', row.genre)